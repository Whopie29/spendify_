# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eGkcecukP27hFT_X6iaIr3KyNJ_lPmGh
"""

#pip install pdfplumber

#pip install pypdf

import os
# Set environment variables to suppress TensorFlow warnings
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend to avoid GUI threading issues
from matplotlib import pyplot as plt
from pypdf import PdfReader, PdfWriter
import pdfplumber
import csv
import re
import os
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from statsmodels.tsa.arima.model import ARIMA
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

# Suppress TensorFlow warnings
import warnings
warnings.filterwarnings('ignore', category=UserWarning, module='tensorflow')
warnings.filterwarnings('ignore', category=FutureWarning, module='tensorflow')

# Ensure NLTK downloads are available
nltk_data_path = os.path.expanduser('~/AppData/Roaming/nltk_data')

if not os.path.exists(nltk_data_path):
    nltk.download('punkt')
    nltk.download('stopwords')

# Bank Configuration System
BANK_CONFIGS = {
    'HDFC': {
        'name': 'HDFC Bank',
        'columns': {
            'date': 'Date',
            'narration': 'Narration',
            'cheque_ref': 'Chq. / Ref No.',
            'withdrawal': 'Withdrawal Amount',
            'deposit': 'Deposit Amount',
            'balance': 'Closing Balance*'
        },
        'date_format': '%d/%m/%Y',
        'amount_columns': ['Withdrawal Amount', 'Deposit Amount', 'Closing Balance*'],
        'skip_rows': 0,
        'header_row': 0
    },
    'SBI': {
        'name': 'State Bank of India',
        'columns': {
            'date': 'Value Dt',
            'narration': 'Transaction Remarks',
            'cheque_ref': 'Cheque Number',
            'withdrawal': 'Withdrawal Amt.',
            'deposit': 'Deposit Amt.',
            'balance': 'Balance'
        },
        'date_format': '%d/%m/%Y',
        'amount_columns': ['Withdrawal Amt.', 'Deposit Amt.', 'Balance'],
        'skip_rows': 0,
        'header_row': 0
    },
    'KOTAK': {
        'name': 'Kotak Mahindra Bank',
        'columns': {
            'date': 'DATE',
            'narration': 'TRANSACTION DETAILS',
            'cheque_ref': 'CHEQUE/REFERENCE#',
            'withdrawal': 'DEBIT',
            'deposit': 'CREDIT',
            'balance': 'BALANCE'
        },
        'date_format': '%d/%m/%Y',
        'amount_columns': ['DEBIT', 'CREDIT', 'BALANCE'],
        'skip_rows': 0,
        'header_row': 0
    },


}

def detect_bank(df):
    """
    Automatically detect the bank based on column names in the dataframe
    """
    columns_lower = [col.lower().strip() for col in df.columns]
    actual_columns = [col.strip() for col in df.columns]
    
    # Check for unique bank-specific column patterns first
    if 'closing balance*' in columns_lower:
        return 'HDFC'
    elif 'value dt' in columns_lower:
        return 'SBI'
    elif 'cheque/reference#' in columns_lower or 'transaction details' in columns_lower:
        return 'KOTAK'

    
    # Check for bank name mentions in columns
    for col in df.columns:
        col_lower = col.lower()
        if 'hdfc' in col_lower:
            return 'HDFC'
        elif 'sbi' in col_lower:
            return 'SBI'
        elif 'kotak' in col_lower:
            return 'KOTAK'


    
    # Fallback: try to match exact column patterns
    for bank, config in BANK_CONFIGS.items():
        bank_columns = list(config['columns'].values())
        if all(col in actual_columns for col in bank_columns):
            return bank
    
    # Default to HDFC if no match found
    return 'HDFC'

def validate_bank_statement(df, bank_code):
    """
    Validate that the statement columns match the expected format for the selected bank
    Returns (is_valid, error_message)
    """
    if bank_code not in BANK_CONFIGS:
        return False, f"Unsupported bank code: {bank_code}"
    
    config = BANK_CONFIGS[bank_code]
    expected_columns = list(config['columns'].values())
    actual_columns = [col.strip() for col in df.columns]  # Strip whitespace
    
    # Check if all expected columns are present
    missing_columns = [col for col in expected_columns if col not in actual_columns]
    
    if missing_columns:
        detected_bank = detect_bank(df)
        detected_config = BANK_CONFIGS[detected_bank]
        return False, f"This appears to be a {detected_config['name']} statement, but you selected {config['name']}. Please select the correct bank or use 'Auto Detect Bank'."
    
    # Check for columns that clearly belong to other banks
    conflicting_banks = []
    for col in actual_columns:
        if col in expected_columns:
            continue
        
        # Check if this column uniquely identifies another bank
        for other_bank, other_config in BANK_CONFIGS.items():
            if other_bank != bank_code:
                other_columns = list(other_config['columns'].values())
                if col in other_columns and col not in expected_columns:
                    conflicting_banks.append(other_config['name'])
                    break
    
    if conflicting_banks:
        unique_banks = list(set(conflicting_banks))
        if len(unique_banks) == 1:
            return False, f"This appears to be a {unique_banks[0]} statement, but you selected {config['name']}. Please select the correct bank or use 'Auto Detect Bank'."
        else:
            return False, f"Statement format doesn't match {config['name']}. Please select the correct bank or use 'Auto Detect Bank'."
    
    return True, "Statement format is valid for the selected bank"

def standardize_columns(df, bank_code):
    """
    Standardize column names to a common format for processing
    """
    config = BANK_CONFIGS[bank_code]
    column_mapping = config['columns']
    
    # Create standardized column names
    standardized_df = df.copy()
    standardized_df.columns = [col.strip() for col in standardized_df.columns]
    
    # Map to standard column names
    standard_columns = {
        'Date': column_mapping['date'],
        'Narration': column_mapping['narration'],
        'Chq. / Ref No.': column_mapping['cheque_ref'],
        'Withdrawal Amount': column_mapping['withdrawal'],
        'Deposit Amount': column_mapping['deposit'],
        'Closing Balance*': column_mapping['balance']
    }
    
    # Rename columns to standard format
    for standard_name, original_name in standard_columns.items():
        if original_name in standardized_df.columns:
            standardized_df = standardized_df.rename(columns={original_name: standard_name})
    
    return standardized_df

class AccountManagementAnalyzer:
    def __init__(self):
        self.df = None
        self.csv_file = None
        self.prediction_df = None
        self.bank_code = None
        self.bank_config = None

    def remove_pdf_password(self, input_pdf, password):
        output_pdf = os.path.splitext(input_pdf)[0] + "_unprotected.pdf"
        try:
            reader = PdfReader(input_pdf)
            
            # Check if PDF is encrypted
            if reader.is_encrypted:
                if not password:  # No password provided for encrypted PDF
                    raise ValueError("This PDF is password protected. Please enter the password.")
                
                if reader.decrypt(password) == 1:
                    writer = PdfWriter()
                    for page in reader.pages:
                        writer.add_page(page)
                    with open(output_pdf, "wb") as f:
                        writer.write(f)
                    print("Password removed successfully!")
                    return output_pdf
                else:
                    raise ValueError("Incorrect password provided.")
            else:
                # PDF is not encrypted, return original file
                print("PDF is not password protected.")
                return input_pdf
                
        except ValueError:
            raise  # Re-raise ValueError for proper error handling
        except Exception as e:
            print("An error occurred:", e)
            return None

    def analyze(self, pdf_path):
        try:
            default_csv_path = os.path.splitext(pdf_path)[0] + ".csv"
            with pdfplumber.open(pdf_path) as pdf, open(default_csv_path, "w", newline="") as f:
                writer = csv.writer(f)
                for page in pdf.pages:
                    table = page.extract_table()
                    if table:
                        writer.writerows(table)
            print("PDF converted to CSV successfully! Saved as:", default_csv_path)
            self.csv_file = default_csv_path
            
            # Load the data and detect bank (but don't set bank_code yet)
            temp_df = pd.read_csv(default_csv_path)
            detected_bank = detect_bank(temp_df)
            detected_config = BANK_CONFIGS[detected_bank]
            print(f"Detected bank: {detected_config['name']}")
            
            # Store detected bank for later use
            self._detected_bank = detected_bank
            self._detected_config = detected_config
            
            return default_csv_path
        except Exception as e:
            print("An error occurred:", e)
            return None

    def show_data(self):
        self.df = pd.read_csv(self.csv_file)
        
        # If no bank is set, use detected bank
        if not self.bank_code:
            if hasattr(self, '_detected_bank'):
                self.bank_code = self._detected_bank
                self.bank_config = self._detected_config
            else:
                # Fallback: detect bank from current data
                self.bank_code = detect_bank(self.df)
                self.bank_config = BANK_CONFIGS[self.bank_code]
        
        # Validate bank statement format against the selected bank
        is_valid, error_message = validate_bank_statement(self.df, self.bank_code)
        if not is_valid:
            raise ValueError(f"❌ Bank Statement Validation Error: {error_message}")
        
        # Standardize columns based on selected bank
        self.df = standardize_columns(self.df, self.bank_code)
        if self.bank_config:
            print(f"✅ Columns standardized for {self.bank_config['name']}")
        else:
            print(f"✅ Columns standardized for bank code: {self.bank_code}")
        
        return self.df

    def info(self):
        return self.df.info()

    def naming_narration(self):
        # Convert to string first to handle different data types
        self.df['Narration'] = self.df['Narration'].astype(str)
        
        # Only apply UPI extraction for HDFC (keep HDFC behavior unchanged)
        if self.bank_code == 'HDFC':
            self.df['Narration'] = self.df['Narration'].str.extract(r'UPI-([^-]+)', expand=False).str.strip().str.title()
        else:
            # For other banks, just clean the narration
            self.df['Narration'] = self.df['Narration'].str.strip().str.title()
        
        return self.df['Narration']

    def narration_csv(self):
        narration_file_path = "narration.csv"
        narration_df = pd.DataFrame(self.df['Narration'])
        narration_df.to_csv(narration_file_path, index=False)
        return narration_file_path

    def preprocessing_and_analysis(self):
       # global df
      # Include your data preprocessing and analysis code here
      
      # Convert to string first to handle different data types
      self.df["Withdrawal Amount"] = self.df["Withdrawal Amount"].astype(str)
      self.df["Deposit Amount"] = self.df["Deposit Amount"].astype(str)
      self.df["Closing Balance*"] = self.df["Closing Balance*"].astype(str)
      
      # Standard withdrawal/deposit processing
      self.df["Withdrawal Amount"] = pd.to_numeric(self.df["Withdrawal Amount"].str.replace(",", ""), errors="coerce").fillna(0)
      self.df["Deposit Amount"] = pd.to_numeric(self.df["Deposit Amount"].str.replace(",", ""), errors="coerce").fillna(0)
      
      self.df["Closing Balance*"] = pd.to_numeric(self.df["Closing Balance*"].str.replace(",", ""), errors="coerce")
      
      # Handle different date formats
      try:
          # Convert Date to string first
          self.df["Date"] = self.df["Date"].astype(str)
          if self.bank_config is not None and 'date_format' in self.bank_config:
              self.df["Date"] = pd.to_datetime(self.df["Date"], format=self.bank_config['date_format'])
          else:
              self.df["Date"] = pd.to_datetime(self.df["Date"], dayfirst=True)
      except:
          # Fallback to default parsing
          self.df["Date"] = pd.to_datetime(self.df["Date"], dayfirst=True)

      self.naming_narration()
      
      # Rename column for consistency
      self.df.rename(columns={'Closing Balance*': 'Closing Balance'}, inplace=True)
      
      # Use enhanced graph generation
      from enhanced_graphs import create_enhanced_graphs
      create_enhanced_graphs(self)
      
      pass


    nltk.download('punkt')
    nltk.download('stopwords')
    nltk.download('punkt_tab')
    def classification(self):
      # Include your transaction classification code here
      self.df['Narration'] = self.df['Narration'].astype(str).fillna('')

      categories = {
        "Food/Clothing": [
            "zomato", "swiggy", "flipkart", "groceries", "amazon", "myntra", "ajio","grofers",
            "nike", "adidas", "zepto", "bigbasket", "dmart", "reliance fresh", "spencers", "foodpanda",
            "kfc", "mcdonalds", "dominos", "pizzahut", "subway", "burger king", "fbb", "pantaloons", "westside","Departmnt Store"
        ],
        "Entertainment": [
            "netflix", "prime", "hotstar", "spotify", "google", "book my show", "jiocinema",
            "hulu", "disney", "sony liv", "voot", "zee5", "itunes", "youtube premium", "audible", "gaana", "wynk"
        ],
        "Recharge": [
            "airtel", "jio", "vodafone", "bsnl", "recharge", "top-up", "vi", "talktime", "prepaid", "postpaid",
            "mobile recharge", "phonepe recharge", "paytm recharge","mobikwik recharge","Paytm"
        ],
        "Rent/Bills": [
            "electricity", "water bill", "rent", "utility", "phonepe", "bbpsbp", "landlord",
            "property tax", "maintenance", "gas bill", "internet bill", "dth recharge", "municipal tax"
        ],
        "Transport": [
            "rapido", "ola", "uber", "metro", "rail", "bus", "flight", "taxi", "redbus", "irctc",
            "indigo", "air india", "spicejet", "go air", "blablacar", "cab", "rickshaw", "fuel", "petrol", "diesel"
        ],
        "Emergency": [
            "hospital", "doctor", "medical", "pharmacy", "cash deposit", "emergency", "ambulance",
            "surgery", "clinic", "meds", "medlife", "pharmeasy", "apollo", "fortis", "max healthcare"
        ],
        "Banking": [
            "airtel payments bank", "navi technologies", "banking", "loan", "credit card", "debit card",
            "upi", "neft", "imps", "rtgs", "interest", "savings", "hdfc", "icici", "sbi", "axis bank", "kotak"
        ],
        "Gaming": [
            "steam", "epic games", "pubg", "game", "valorant", "counter strike", "call of duty", "roblox",
            "playstation", "xbox", "nintendo", "esports", "minecraft", "rummy", "poker", "fantasy cricket"
        ],
        "Trading": [
            "zerodha", "upstox", "angel broking", "groww", "stocks", "equity", "mutual funds",
            "investment", "bonds", "nse", "bse", "cryptocurrency", "bitcoin", "forex", "commodities", "shares"
        ],
        "Personal Transfer": []  #
      }
      stop_words = set(stopwords.words('english'))



      #classify transaction
      def classify_transaction(text):
        text = text.lower()
        words = word_tokenize(text)
        words = [word for word in words if word.isalnum() and word not in stop_words]

        for category, keywords in categories.items():
            if any(keyword in text for keyword in keywords):
                return category

        return "Personal Transfer"



      self.df['Category'] = self.df['Narration'].apply(classify_transaction)

      self.df.to_csv("classified_narrations.csv", index=False)

      self.df
      #np.savetxt("classified_narrations.csv", self.df.values, delimiter=",", fmt="%s")


      # Category distribution graph is now handled in enhanced_graphs.py
      # during preprocessing_and_analysis() call

      pass

    def trans_pred(self, future_days=30):
        # Include your transaction prediction code here

      #global df, prediction_df

      # processing the data around the dates
      try:
          if self.bank_config and 'date_format' in self.bank_config:
              self.df["Date"] = pd.to_datetime(self.df["Date"], format=self.bank_config['date_format'])
          else:
              self.df["Date"] = pd.to_datetime(self.df["Date"], format="%d/%m/%Y")
      except:
          # Fallback to default parsing
          self.df["Date"] = pd.to_datetime(self.df["Date"], format="%d/%m/%Y")

      self.df["Withdrawal Amount"] = pd.to_numeric(self.df["Withdrawal Amount"].astype(str).replace(",", ""), errors="coerce").fillna(0).astype(float)
      self.df["Deposit Amount"] = pd.to_numeric(self.df["Deposit Amount"].astype(str).replace(",", ""), errors="coerce").fillna(0).astype(float)
      self.df["Closing Balance"] = pd.to_numeric(self.df["Closing Balance"].astype(str).replace(",", ""), errors="coerce").fillna(0).astype(float)


      # Sort by Date
      self.df = self.df.sort_values("Date")

      # Create time series data
      data = self.df[["Closing Balance"]].values
      scaler = MinMaxScaler(feature_range=(0, 1))
      data_scaled = scaler.fit_transform(data)

      # LSTM MODEL
      def create_sequences(data, seq_length):
          X, y = [], []
          for i in range(len(data) - seq_length):
              X.append(data[i:i+seq_length])
              y.append(data[i+seq_length])
          return np.array(X), np.array(y)

      # sequence
      seq_length = 5  # Use past 5 days to predict next day
      X, y = create_sequences(data_scaled, seq_length)


      # Train-test split
      train_size = int(len(X) * 0.8)
      X_train, y_train = X[:train_size], y[:train_size]
      X_test, y_test = X[train_size:], y[train_size:]

      # LSTM Model
      model = Sequential([
          LSTM(50, return_sequences=True),
          LSTM(50),
          Dense(1)
      ])
      model.compile(optimizer='adam', loss='mse')
      model.fit(X_train, y_train, epochs=50, batch_size=16, verbose=1)


      # Predict future balance
      predictions = []
      last_sequence = X_test[-1]
      for _ in range(future_days):
          next_day_pred = model.predict(last_sequence.reshape(1, seq_length, 1))
          predictions.append(next_day_pred[0, 0])
          last_sequence = np.vstack((last_sequence[1:], next_day_pred))

      predicted_values = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))



      # ARIMA Model
      model_arima = ARIMA(self.df["Closing Balance"], order=(5,1,0))
      model_arima_fit = model_arima.fit()
      forecast_arima = model_arima_fit.forecast(steps=future_days)


      # Prepare future dataframe
      dates_future = pd.date_range(self.df["Date"].iloc[-1], periods=future_days+1)[1:]
      prediction_df = pd.DataFrame({
          "Date": dates_future,
          "LSTM_Prediction": predicted_values.flatten(),
          "ARIMA_Prediction": forecast_arima
      })

      self.prediction_df = prediction_df

      # comparing both model and predict balance after n days
      current_balance = self.df["Closing Balance"].iloc[-1]

      print(f"Current Balance: ₹{current_balance:.2f}")
      print("\nFuture Predictions (ARIMA & LSTM):\n", self.prediction_df)
      
      # analysing error of model
      actual_balances = scaler.inverse_transform(y_test)
      lstm_predictions = scaler.inverse_transform(model.predict(X_test))
      
      # Slice the forecast to match the length of the test set for metric calculation
      arima_predictions_for_metrics = forecast_arima[:len(y_test)]
      
      # Ensure arrays for metrics have the same size to avoid broadcasting errors
      metric_len = len(arima_predictions_for_metrics)
      actual_balances_flat = actual_balances.flatten()[:metric_len]
      lstm_predictions_flat = lstm_predictions.flatten()[:metric_len]

      msme_lstm = np.mean((actual_balances_flat - lstm_predictions_flat) ** 2) / np.mean(actual_balances_flat ** 2)
      msme_arima = np.mean((actual_balances_flat - arima_predictions_for_metrics) ** 2) / np.mean(actual_balances_flat ** 2)

      from sklearn.metrics import mean_absolute_error
      mae_lstm = mean_absolute_error(actual_balances_flat, lstm_predictions_flat)
      rmae_lstm = mae_lstm / np.mean(actual_balances_flat)

      mae_arima = mean_absolute_error(actual_balances_flat, arima_predictions_for_metrics)
      rmae_arima = mae_arima / np.mean(actual_balances_flat)

      print(f"MSME_LSTM:{msme_lstm}")
      print(f"MSME_ARIMA:{msme_arima}")
      print(f"RMAE_LSTM:{rmae_lstm}")
      print(f"RMAE_ARIMA:{rmae_arima}")

      return current_balance, self.prediction_df

    def budget_system(self):
      # Include your budgeting system code here

      #global prediction_df
    # setting the classification dataset

      cat = pd.read_csv("classified_narrations.csv") # add header=None
      
      # Use existing column names instead of hardcoded ones
      # This prevents column mismatch errors for different banks
      if len(cat.columns) != 8:
          # If columns don't match expected count, use the CSV as-is
          pass
      else:
          column_names = ["Date", "Narration", "Chq. / Ref No.", "Value Date",
                          "Withdrawal Amount", "Deposit Amount", "Closing Balance", "Category"]
          cat.columns = column_names


      # model mean (ARIMA and LSTM)
      self.prediction_df["LSTM_Prediction"] = pd.to_numeric(self.prediction_df["LSTM_Prediction"], errors='coerce')  # Convert to numeric, handle errors
      self.prediction_df["ARIMA_Prediction"] = pd.to_numeric(self.prediction_df["ARIMA_Prediction"], errors='coerce')

      predicted_income = self.prediction_df["LSTM_Prediction"].mean()
      predicted_expense = self.prediction_df["ARIMA_Prediction"].mean()


      # AI-Based Budgeting Breakdown
      predicted_savings = predicted_income * 0.2  # 20% of income as savings
      essential_expense_pred = predicted_expense * 0.7  # 70% on essentials
      non_essential_expense_pred = predicted_expense * 0.3  # 30% on non-essentials

      # personal transfer not required
      filtered_df = cat[cat["Category"] != "Personal Transfer"]

      # Use 'Withdrawal Amount' instead of 'Withdrawal_Amount'
      category_expense = filtered_df.groupby("Category")["Withdrawal Amount"].sum()

      # setting the past expense and predicted expense
      past_expense_ratio = category_expense / category_expense.sum()
      adaptive_allocation = predicted_expense * past_expense_ratio

      # conditions for expense (predicted and actual)
      if predicted_expense > predicted_income * 0.8:
        savings_ratio = 0.1
      else:
          savings_ratio = 0.2

      dynamic_savings = predicted_income * savings_ratio
      essential_expense = predicted_expense * 0.7
      non_essential_expense = predicted_expense * 0.3

      # Use enhanced budget graph generation
      from enhanced_graphs import create_budget_graphs
      create_budget_graphs(category_expense, dynamic_savings, essential_expense, non_essential_expense)

      # Check for overspending to generate warnings
      over_spending = adaptive_allocation[adaptive_allocation > category_expense]
      over_spending_dict = over_spending.to_dict() if not over_spending.empty else None
      
      income_warning = None
      if predicted_expense > predicted_income:
          overspending_amount = predicted_expense - predicted_income
          income_warning = f"Warning: Your predicted expenses exceed your income by ₹{overspending_amount:.2f}. Consider reducing discretionary spending."

      # Return a dictionary with all budget details
      return {
          'predicted_income': predicted_income,
          'predicted_expense': predicted_expense,
          'savings_ratio': savings_ratio,
          'dynamic_savings': dynamic_savings,
          'essential_expense': essential_expense,
          'non_essential_expense': non_essential_expense,
          'category_expense': category_expense.to_dict(),
          'adaptive_allocation': adaptive_allocation.to_dict(),
          'over_spending': over_spending_dict,
          'income_warning': income_warning
      }

if __name__ == "__main__":
    print("🚀 SPENDIFY Multi-Bank Financial Analyzer")
    print("=" * 50)
    print("This is a library module. To run the application, use:")
    print("• Flask version: python app.py")
    print("• Streamlit version: streamlit run web.py")
    print("• Test multi-bank support: python test_multi_bank.py")
    print("• Demo multi-bank functionality: python demo_multi_bank.py")
    print("\nFor direct usage, import the AccountManagementAnalyzer class:")
    print("from main import AccountManagementAnalyzer")
    print("analyzer = AccountManagementAnalyzer()")